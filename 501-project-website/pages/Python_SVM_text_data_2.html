<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.242">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The SVM Method for Twitter API Text Data (NLP)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="Python_SVM_text_data_2_files/libs/clipboard/clipboard.min.js"></script>
<script src="Python_SVM_text_data_2_files/libs/quarto-html/quarto.js"></script>
<script src="Python_SVM_text_data_2_files/libs/quarto-html/popper.min.js"></script>
<script src="Python_SVM_text_data_2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Python_SVM_text_data_2_files/libs/quarto-html/anchor.min.js"></script>
<link href="Python_SVM_text_data_2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Python_SVM_text_data_2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Python_SVM_text_data_2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Python_SVM_text_data_2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Python_SVM_text_data_2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">The SVM Method for Twitter API Text Data (NLP)</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="1">
<div class="cell-output cell-output-display" data-execution_count="1">
<p><img src="Python_SVM_text_data_2_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><em>Picture from BLOG</em></p>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="what-is-svmsupport-vector-machine-how-it-works" class="level3">
<h3 class="anchored" data-anchor-id="what-is-svmsupport-vector-machine-how-it-works">What is SVM(Support Vector Machine)? How it works?</h3>
<p>As a Data Science graduate student and a machine learning enthusiast, SVM is a helpful method in machine learning. For example, if I just have a bunch of data that needs to be solved by SVM method, either Matlab’s SVM toolbox, or SciKitLearn in python can provide a convenient and quick solution.</p>
<p>But if we want to understand SVM, we will begin with “KKT…”, “Lagrangian …”, “Maximum interval” and so on, these terms tend to follow a whole bunch of heavenly formulas, making us feel dizzy for a while. Not to mention how to connect from one formula to another to understand SVM.</p>
<p>During training process, the SVM learns the importance of each data point for representing the decision boundary between two categories. Usually only those points that lie on the boundary between the categories are important for defining the decision boundary. These points are called support vectors. If we want to make a prediction for a new sample point, we need to measure the distance between this point and each support vector. The classification decision is made based on its distance from the support vector and the importance of the support vector learned during the training process.</p>
<p>Generally speaking, SVM is a method that can divide the samples into two categories！</p>
</section>
<section id="code-and-methodology" class="level3">
<h3 class="anchored" data-anchor-id="code-and-methodology">Code and Methodology</h3>
<p>I will use Twitter Data, which has 1000+ rows of Text data with Label “0” and “1”. Label column has 0 for a positive attitude and 1 for a negative attitude. I will use two columns named “final_tweets” and “label” for SVM classification.</p>
<section id="step-1-library-packages" class="level4">
<h4 class="anchored" data-anchor-id="step-1-library-packages">Step 1: Library packages</h4>
<p>Library packages for preparing. I will use nltk package for text data cleaning and sklearn package for svm classification.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> pos_tag</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet <span class="im">as</span> wn</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection, naive_bayes, svm</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="step-2-set-random-seed" class="level4">
<h4 class="anchored" data-anchor-id="step-2-set-random-seed">Step 2: Set random seed</h4>
<p>Set random seed to keep the same result.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1234</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="step-3-import-text-data" class="level4">
<h4 class="anchored" data-anchor-id="step-3-import-text-data">Step 3: Import text data</h4>
<p>Importing text data using relative file paths, then only use “final_twitter” and “label” column for classfication. Then I take the first 5 rows of data to see if the criteria are met.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>txt_data <span class="op">=</span> pd.read_csv(<span class="st">"../../data/01-modified-data/cleaned_twitter.csv"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>txt_data.iloc[:,[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<div id="tbl-planets" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;1:  first 5 row of twitter data </caption>
  <thead>
    <tr>
      <th></th>
      <th>final_tweets</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>TxCompleted I think Id lying I put anything ca...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>satoshinftclub WHITELIST amp ETH GIVEAWAYx Whi...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>wl Notice Minting eleveneth cryptobrighton Avn...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>MoonlitMonkey THOmaximalist THOChain You eth Y...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Did know On average BTC million mined daily</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>sns.displot(data[<span class="st">'label'</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-polar" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Python_SVM_text_data_2_files/figure-html/fig-polar-output-1.png" width="469" height="468" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Lable Distribution</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Figure-1 shows the frequency of tweets with negative attitudes and tweets with positive attitudes. The number of tweets with positive attitudes is much higher than the number of tweets with negative attitudes, and the number of positive attitudes is about 6 times higher than the number of negative attitudes. This shows that although the overall market trend of crypto is not stable, the majority of people still have a positive attitude towards cryptocurrency from the data.</p>
</section>
<section id="step-4-normalized-data" class="level4">
<h4 class="anchored" data-anchor-id="step-4-normalized-data">Step 4: Normalized data</h4>
<p>I had cleaned the data before, but when I looked at the data, I still need to change all the text to lower case. This is required as python interprets ‘cat’ and ‘CAT’ differently. And Tokenization: In this each entry in the data will be broken into set of words, which is more conducive to subsequent analysis, Remove Stop words and WordNetLemmatizer. the final cleaned data.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>].dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data.final_tweets.<span class="bu">str</span>.lower()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>] <span class="op">=</span>data[<span class="st">'final_tweets'</span>].astype(<span class="bu">str</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>]<span class="op">=</span> [word_tokenize(entry) <span class="cf">for</span> entry <span class="kw">in</span> data[<span class="st">'final_tweets'</span>]]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>tag_map <span class="op">=</span> defaultdict(<span class="kw">lambda</span> : wn.NOUN)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'J'</span>] <span class="op">=</span> wn.ADJ</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'V'</span>] <span class="op">=</span> wn.VERB</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'R'</span>] <span class="op">=</span> wn.ADV</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index,entry <span class="kw">in</span> <span class="bu">enumerate</span>(data[<span class="st">'final_tweets'</span>]):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    Final_words <span class="op">=</span> []</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    word_Lemmatized <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, tag <span class="kw">in</span> pos_tag(entry):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stopwords.words(<span class="st">'english'</span>) <span class="kw">and</span> word.isalpha():</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            word_Final <span class="op">=</span> word_Lemmatized.lemmatize(word,tag_map[tag[<span class="dv">0</span>]])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            Final_words.append(word_Final)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    data.loc[index,<span class="st">'text_final'</span>] <span class="op">=</span> <span class="bu">str</span>(Final_words)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>data.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
<div id="tbl-planets1" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;2:  first 5 row of twitter final cleaned data </caption>
  <thead>
    <tr>
      <th></th>
      <th>final_tweets</th>
      <th>label</th>
      <th>text_final</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[TxCompleted, I, think, Id, lying, I, put, any...</td>
      <td>0</td>
      <td>['TxCompleted', 'I', 'think', 'Id', 'lie', 'I'...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[satoshinftclub, WHITELIST, amp, ETH, GIVEAWAY...</td>
      <td>0</td>
      <td>['satoshinftclub', 'WHITELIST', 'amp', 'ETH', ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[wl, Notice, Minting, eleveneth, cryptobrighto...</td>
      <td>0</td>
      <td>['wl', 'Notice', 'Minting', 'eleveneth', 'cryp...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[MoonlitMonkey, THOmaximalist, THOChain, You, ...</td>
      <td>0</td>
      <td>['MoonlitMonkey', 'THOmaximalist', 'THOChain',...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[Did, know, On, average, BTC, million, mined, ...</td>
      <td>1</td>
      <td>['Did', 'know', 'On', 'average', 'BTC', 'milli...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="step-5-split-train-data-and-text-data" class="level4">
<h4 class="anchored" data-anchor-id="step-5-split-train-data-and-text-data">Step 5: Split Train data and Text data</h4>
<p>Twitter data will be split into two datasets, train data and test data. The train data will be used to fit the model and the test dataset will be used to predict. I will use train_test_split function in the sklearn library. I set the parameter test_size=0.2, which means that the training data will have 80% of the corpus and the test data will have the remaining 20%.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> model_selection.train_test_split(data[<span class="st">'text_final'</span>],data[<span class="st">'label'</span>],test_size<span class="op">=</span><span class="fl">0.2</span>,random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
</section>
<section id="baseline-model-for-comparison-feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="baseline-model-for-comparison-feature-selection">Baseline Model for Comparison &amp; Feature Selection</h2>
<p><strong>Random Classifer to prdict y_test:</strong></p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_label_data(class_labels, weights,N<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>random.choices(class_labels, weights <span class="op">=</span> weights, k <span class="op">=</span> N)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>all_len<span class="op">=</span><span class="bu">len</span>(y_test)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>len_0<span class="op">=</span><span class="bu">len</span>(y_test[y_test<span class="op">==</span><span class="dv">0</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>len_1<span class="op">=</span><span class="bu">len</span>(y_test[y_test<span class="op">==</span><span class="dv">1</span>])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>pro_label_y_0<span class="op">=</span>len_0<span class="op">/</span>all_len</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>pro_label_y_1<span class="op">=</span>len_1<span class="op">/</span>all_len</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>generate_label_data([<span class="dv">0</span>,<span class="dv">1</span>],[pro_label_y_0,pro_label_y_1],all_len)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> y</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> y_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The accuracy after running the random classifier is 0.75.</p>
<p>Display the classification report for test data to random classifer.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>class_report_test_random<span class="op">=</span>classification_report(y_true,y_pred, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>class_report_test_random<span class="op">=</span>pd.DataFrame(class_report_test_random).transpose()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>class_report_test_random</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<div id="tbl-planets9" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;3:  the classification report for test data-random classifer </caption>
  <thead>
    <tr>
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.844444</td>
      <td>0.868571</td>
      <td>0.856338</td>
      <td>175.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.115385</td>
      <td>0.096774</td>
      <td>0.105263</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.752427</td>
      <td>0.752427</td>
      <td>0.752427</td>
      <td>0.752427</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.479915</td>
      <td>0.482673</td>
      <td>0.480801</td>
      <td>206.000000</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.734732</td>
      <td>0.752427</td>
      <td>0.743312</td>
      <td>206.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="feature-selection-model-tuning" class="level2">
<h2 class="anchored" data-anchor-id="feature-selection-model-tuning">Feature Selection &amp; Model Tuning</h2>
<p>I transformed the text data into vectors that the model can understand, by using the TF-IDF approach. The following code can be used to first fit the TG-IDF model across the data. This will help TF-IDF build a vocabulary that it learns from the data and will assign a unique integer to each word. I set the parameter max_features=5000, which means the vocabulary will have a maximum of 5000 unique words/features. I converted X_train，X_test to vectors X_train_Tfidf and X_test_Tfidf. After the transformation, the dataset can be fed into the SVM classification algorithm.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Tfidf_vect <span class="op">=</span> TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Tfidf_vect.fit(data[<span class="st">'text_final'</span>])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>X_train_Tfidf <span class="op">=</span> Tfidf_vect.transform(X_train)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X_test_Tfidf <span class="op">=</span> Tfidf_vect.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Set a function to calculate ACCURACY, Recall and precision score.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data,y_pred):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"ACCURACY:"</span>, accuracy_score(y_data,y_pred))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"NEGATIVE RECALL (Y=0):"</span>,recall_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"NEGATIVE PRECISION (Y=0):"</span>,precision_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"POSITIVE RECALL (Y=1):"</span>,recall_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"POSITIVE PRECISION (Y=1):"</span>,precision_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="final-results" class="level2">
<h2 class="anchored" data-anchor-id="final-results">Final Results</h2>
<p>SVM with Linear kernels: The linear kernel, which produces linear classification bounds, is generally the most computationally efficient and requires the least data. linear function. Fit the classifier on the training data and predict on the test data. Set the classifier to be linear and C equal to 0.5.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="fl">0.5</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>model_linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>,C<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>model_linear.fit(X_train_Tfidf, y_train)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model_linear.predict(X_test_Tfidf)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM TEST DATA SUMMARY-Linear Kernels"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>confusion_plot(yp_test, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SVM TEST DATA SUMMARY-Linear Kernels
ACCURACY: 0.9271844660194175
NEGATIVE RECALL (Y=0): 0.9210526315789473
NEGATIVE PRECISION (Y=0): 1.0
POSITIVE RECALL (Y=1): 1.0
POSITIVE PRECISION (Y=1): 0.5161290322580645</code></pre>
</div>
</div>
<p>Display the classification report for test data.</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>class_report_test<span class="op">=</span>classification_report(y_test,yp_test, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>class_report_test<span class="op">=</span>pd.DataFrame(class_report_test).transpose()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>class_report_test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<div id="tbl-planets2" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;4:  the classification report for test data(linear) </caption>
  <thead>
    <tr>
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.921053</td>
      <td>1.000000</td>
      <td>0.958904</td>
      <td>175.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.000000</td>
      <td>0.516129</td>
      <td>0.680851</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.927184</td>
      <td>0.927184</td>
      <td>0.927184</td>
      <td>0.927184</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.960526</td>
      <td>0.758065</td>
      <td>0.819878</td>
      <td>206.000000</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.932933</td>
      <td>0.927184</td>
      <td>0.917061</td>
      <td>206.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<p>Display Confusion Matrix for the test data.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>cm_test<span class="op">=</span>confusion_matrix(y_test,yp_test)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>disp_test <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm_test,display_labels<span class="op">=</span>model_linear.classes_)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>disp_test.plot()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-polar1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Python_SVM_text_data_2_files/figure-html/fig-polar1-output-1.png" width="497" height="422" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: confusion matrix for test data(linear)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From the classification report for test data and the consusion matrix, the accuracy of this SVM model with linear kernal is 0.927184, which is very high.</p>
<p>Although SVM usually performs well, it is very sensitive to parameter settings and data scaling. If you want to improve the model accuracy, I try rbf kenel, radial basis function, also known as Gaussian kernel, which is based on the distance to each support vector to determine the classification boundary, it can map to infinite dimensions and is the most flexible method, but also requires the most data. It is prone to overfitting problems. Exponential function. The smaller the C value, the stronger the regularization effect. The more trained the model is more generalized, but also more likely to be under-fitted. Hence, I choose the value of c is 1000.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model_rbf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"rbf"</span>,C<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>model_rbf.fit(X_train_Tfidf, y_train)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>yp_test_rbf <span class="op">=</span> model_rbf.predict(X_test_Tfidf)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM TEST DATA SUMMARY -RBF Kernels"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>confusion_plot(yp_test_rbf, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>SVM TEST DATA SUMMARY -RBF Kernels
ACCURACY: 0.9320388349514563
NEGATIVE RECALL (Y=0): 0.9259259259259259
NEGATIVE PRECISION (Y=0): 1.0
POSITIVE RECALL (Y=1): 1.0
POSITIVE PRECISION (Y=1): 0.5483870967741935</code></pre>
</div>
</div>
<p>Display the classification report for test data.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf<span class="op">=</span>classification_report(y_test,yp_test_rbf, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf<span class="op">=</span>pd.DataFrame(class_report_test_rbf).transpose()</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">
<div id="tbl-planets3" class="anchored">

<div>

<table class="dataframe table table-sm table-striped"><caption>Table&nbsp;5:  the classification report for test data(rbf) </caption>
  <thead>
    <tr>
      <th></th>
      <th>precision</th>
      <th>recall</th>
      <th>f1-score</th>
      <th>support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.925926</td>
      <td>1.000000</td>
      <td>0.961538</td>
      <td>175.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.000000</td>
      <td>0.548387</td>
      <td>0.708333</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>accuracy</th>
      <td>0.932039</td>
      <td>0.932039</td>
      <td>0.932039</td>
      <td>0.932039</td>
    </tr>
    <tr>
      <th>macro avg</th>
      <td>0.962963</td>
      <td>0.774194</td>
      <td>0.834936</td>
      <td>206.000000</td>
    </tr>
    <tr>
      <th>weighted avg</th>
      <td>0.937073</td>
      <td>0.932039</td>
      <td>0.923435</td>
      <td>206.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<p>Display Confusion Matrix for the test data.</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>cm_test_rbf<span class="op">=</span>confusion_matrix(y_test,yp_test_rbf)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>disp_test_rbf <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm_test_rbf,display_labels<span class="op">=</span>model_linear.classes_)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>disp_test_rbf.plot()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-polar2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Python_SVM_text_data_2_files/figure-html/fig-polar2-output-1.png" width="497" height="422" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: confusion matrix for test data(rbf)</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From the classification report for test data and the consusion matrix, the accuracy of this SVM model with rbf kernal is 0.932039, which is slightly higher than the result from the linear model.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>From the above SVM model, we can see that the model is very effective in predicting sentiment analysis. After collecting a certain amount of data, the SVM model is more accurate than the NB model but is relatively time-consuming.</p>
<p>SVM is a very powerful model and performs well on a variety of datasets. SVM allows decision bounds to be complex, even if the data has only a few features. It performs well on data with few features and many features, but does not perform well for scaling the number of samples. Running SVM on data with up to 10,000 samples may perform well, but if the amount of data reaches 1,000,000 rows or even larger, there may be challenges in terms of runtime and memory usage. another disadvantage of SVM is that pre-processing the data and tuning the parameters requires great care. For example, random forests require little or no preprocessing.</p>
<p>Although the cryptocurrency market is highly risky, its advantages such as transaction speed, transaction costs, accessibility, security, etc. still make it a popular investment choice for many investors. Most of the investors are still positive about it.</p>
<section id="pro-and-cons" class="level3">
<h3 class="anchored" data-anchor-id="pro-and-cons">Pro and Cons</h3>
<p><strong>Advantages</strong>: good adaptation to high-dimensional spaces; good performance even when the number of data dimensions is larger than the number of samples; only a subset of the training set is required.</p>
<p><strong>Disadvantages</strong>: SVM performs poorly if the number of features of the data is much larger than the number of samples; SVM does not directly provide quantitative likelihood predictions, these are usually achieved through cross-validation methods.</p>
<!-- -->

</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The SVM Method for Twitter API Text Data (NLP)"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>myImage <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'../../501-project-website/images/SVM/SVM.png'</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>myImage</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>*Picture from BLOG*</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Methods</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is SVM(Support Vector Machine)? How it works?</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>As a Data Science graduate student and a machine learning enthusiast, SVM is a helpful method in machine learning. For example, if I just have a bunch of data that needs to be solved by SVM method, either Matlab's SVM toolbox, or SciKitLearn in python can provide a convenient and quick solution.</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>But if we want to understand SVM, we will begin with "KKT...", "Lagrangian ...", "Maximum interval" and so on, these terms tend to follow a whole bunch of heavenly formulas, making us feel dizzy for a while. Not to mention how to connect from one formula to another to understand SVM.</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>During training process, the SVM learns the importance of each data point for representing the decision boundary between two categories. Usually only those points that lie on the boundary between the categories are important for defining the decision boundary. These points are called support vectors. If we want to make a prediction for a new sample point, we need to measure the distance between this point and each support vector. The classification decision is made based on its distance from the support vector and the importance of the support vector learned during the training process.</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>Generally speaking, SVM is a method that can divide the samples into two categories！</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="fu">### Code and Methodology </span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>I will use Twitter Data, which has 1000+ rows of Text data with Label "0" and "1". Label column has 0 for a positive attitude and 1 for a negative attitude. I will use two columns named "final_tweets" and "label" for SVM classification.</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 1: Library packages </span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>Library packages for preparing. I will use nltk package for text data cleaning and sklearn package for svm classification.</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> pos_tag</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> wordnet <span class="im">as</span> wn</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection, naive_bayes, svm</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> recall_score</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 2: Set random seed</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>Set random seed to keep the same result.</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1234</span>)</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 3: Import text data</span></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>Importing text data using relative file paths, then only use "final_twitter" and "label" column for classfication. Then I take the first 5 rows of data to see if the criteria are met.</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-planets</span></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: first 5 row of twitter data</span></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>txt_data <span class="op">=</span> pd.read_csv(<span class="st">"../../data/01-modified-data/cleaned_twitter.csv"</span>)</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>txt_data.iloc[:,[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>data.head()</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polar</span></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Lable Distribution"</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>sns.displot(data[<span class="st">'label'</span>])</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>Figure-1 shows the frequency of tweets with negative attitudes and tweets with positive attitudes. The number of tweets with positive attitudes is much higher than the number of tweets with negative attitudes, and the number of positive attitudes is about 6 times higher than the number of negative attitudes. This shows that although the overall market trend of crypto is not stable, the majority of people still have a positive attitude towards cryptocurrency from the data.</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 4: Normalized data</span></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>I had cleaned the data before, but when I looked at the data, I still need to change all the text to lower case. This is required as python interprets 'cat' and 'CAT' differently. And Tokenization: In this each entry in the data will be broken into set of words, which is more conducive to subsequent analysis, Remove Stop words and WordNetLemmatizer. the final cleaned data.</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-planets1</span></span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: first 5 row of twitter final cleaned data</span></span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>].dropna(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>data.final_tweets.<span class="bu">str</span>.lower()</span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>] <span class="op">=</span>data[<span class="st">'final_tweets'</span>].astype(<span class="bu">str</span>)</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'final_tweets'</span>]<span class="op">=</span> [word_tokenize(entry) <span class="cf">for</span> entry <span class="kw">in</span> data[<span class="st">'final_tweets'</span>]]</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>tag_map <span class="op">=</span> defaultdict(<span class="kw">lambda</span> : wn.NOUN)</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'J'</span>] <span class="op">=</span> wn.ADJ</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'V'</span>] <span class="op">=</span> wn.VERB</span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>tag_map[<span class="st">'R'</span>] <span class="op">=</span> wn.ADV</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index,entry <span class="kw">in</span> <span class="bu">enumerate</span>(data[<span class="st">'final_tweets'</span>]):</span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>    Final_words <span class="op">=</span> []</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a>    word_Lemmatized <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word, tag <span class="kw">in</span> pos_tag(entry):</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stopwords.words(<span class="st">'english'</span>) <span class="kw">and</span> word.isalpha():</span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a>            word_Final <span class="op">=</span> word_Lemmatized.lemmatize(word,tag_map[tag[<span class="dv">0</span>]])</span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>            Final_words.append(word_Final)</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a>    data.loc[index,<span class="st">'text_final'</span>] <span class="op">=</span> <span class="bu">str</span>(Final_words)</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a>data.head(<span class="dv">5</span>)</span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 5: Split Train data and Text data</span></span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>Twitter data will be split into two datasets, train data and test data. The train data will be used to fit the model and the test dataset will be used to predict. I will use train_test_split function in the sklearn library. I set the parameter test_size=0.2, which means that the training data will have 80% of the corpus and the test data will have the remaining 20%.</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> model_selection.train_test_split(data[<span class="st">'text_final'</span>],data[<span class="st">'label'</span>],test_size<span class="op">=</span><span class="fl">0.2</span>,random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Baseline Model for Comparison &amp; Feature Selection</span></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>**Random Classifer to prdict y_test:**</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_label_data(class_labels, weights,N<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>random.choices(class_labels, weights <span class="op">=</span> weights, k <span class="op">=</span> N)</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y</span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a>all_len<span class="op">=</span><span class="bu">len</span>(y_test)</span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a>len_0<span class="op">=</span><span class="bu">len</span>(y_test[y_test<span class="op">==</span><span class="dv">0</span>])</span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a>len_1<span class="op">=</span><span class="bu">len</span>(y_test[y_test<span class="op">==</span><span class="dv">1</span>])</span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a>pro_label_y_0<span class="op">=</span>len_0<span class="op">/</span>all_len</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a>pro_label_y_1<span class="op">=</span>len_1<span class="op">/</span>all_len</span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>generate_label_data([<span class="dv">0</span>,<span class="dv">1</span>],[pro_label_y_0,pro_label_y_1],all_len)</span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> y</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> y_test</span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>The accuracy after running the random classifier is 0.75.  </span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a>Display the classification report for test data to random classifer.</span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-planets9</span></span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: the classification report for test data-random classifer</span></span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a>class_report_test_random<span class="op">=</span>classification_report(y_true,y_pred, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a>class_report_test_random<span class="op">=</span>pd.DataFrame(class_report_test_random).transpose()</span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>class_report_test_random</span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Selection &amp; Model Tuning</span></span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>I transformed the text data into vectors that the model can understand, by using the TF-IDF approach.</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a>The following code can be used to first fit the TG-IDF model across the data. This will help TF-IDF build a vocabulary that it learns from the data and will assign a unique integer to each word. I set the parameter max_features=5000, which means the vocabulary will have a maximum of 5000 unique words/features. I converted X_train，X_test to vectors X_train_Tfidf and X_test_Tfidf. After the transformation, the dataset can be fed into the SVM classification algorithm.</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a>Tfidf_vect <span class="op">=</span> TfidfVectorizer(max_features<span class="op">=</span><span class="dv">5000</span>)</span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a>Tfidf_vect.fit(data[<span class="st">'text_final'</span>])</span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>X_train_Tfidf <span class="op">=</span> Tfidf_vect.transform(X_train)</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a>X_test_Tfidf <span class="op">=</span> Tfidf_vect.transform(X_test)</span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a>Set a function to calculate ACCURACY, Recall and precision score.</span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confusion_plot(y_data,y_pred):</span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"ACCURACY:"</span>, accuracy_score(y_data,y_pred))</span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"NEGATIVE RECALL (Y=0):"</span>,recall_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"NEGATIVE PRECISION (Y=0):"</span>,precision_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"POSITIVE RECALL (Y=1):"</span>,recall_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"POSITIVE PRECISION (Y=1):"</span>,precision_score(y_data,y_pred,pos_label<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a><span class="fu">## Final Results</span></span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a>SVM with Linear kernels: The linear kernel, which produces linear classification bounds, is generally the most computationally efficient and requires the least data. linear function. Fit the classifier on the training data and predict on the test data. Set the classifier to be linear and C equal to 0.5.</span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="fl">0.5</span> </span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a>model_linear <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"linear"</span>,C<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>model_linear.fit(X_train_Tfidf, y_train)</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>yp_test <span class="op">=</span> model_linear.predict(X_test_Tfidf)</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM TEST DATA SUMMARY-Linear Kernels"</span>)</span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>confusion_plot(yp_test, y_test)</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a>Display the classification report for test data.</span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-planets2</span></span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: the classification report for test data(linear)</span></span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a>class_report_test<span class="op">=</span>classification_report(y_test,yp_test, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a>class_report_test<span class="op">=</span>pd.DataFrame(class_report_test).transpose()</span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>class_report_test</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a>Display Confusion Matrix for the test data.</span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polar1</span></span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "confusion matrix for test data(linear)"</span></span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>cm_test<span class="op">=</span>confusion_matrix(y_test,yp_test)</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>disp_test <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm_test,display_labels<span class="op">=</span>model_linear.classes_)</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a>disp_test.plot()</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>From the classification report for test data and the consusion matrix, the accuracy of this SVM model with linear kernal is 0.927184, which is very high.</span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>Although SVM usually performs well, it is very sensitive to parameter settings and data scaling. If you want to improve the model accuracy, I try rbf kenel, radial basis function, also known as Gaussian kernel, which is based on the distance to each support vector to determine the classification boundary, it can map to infinite dimensions and is the most flexible method, but also requires the most data. It is prone to overfitting problems. Exponential function. The smaller the C value, the stronger the regularization effect. The more trained the model is more generalized, but also more likely to be under-fitted. Hence, I choose the value of c is 1000.</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a>model_rbf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">"rbf"</span>,C<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>model_rbf.fit(X_train_Tfidf, y_train)</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a>yp_test_rbf <span class="op">=</span> model_rbf.predict(X_test_Tfidf)</span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"SVM TEST DATA SUMMARY -RBF Kernels"</span>)</span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a>confusion_plot(yp_test_rbf, y_test)</span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a>Display the classification report for test data.</span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-planets3</span></span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: the classification report for test data(rbf)</span></span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf<span class="op">=</span>classification_report(y_test,yp_test_rbf, output_dict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf<span class="op">=</span>pd.DataFrame(class_report_test_rbf).transpose()</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a>class_report_test_rbf</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a>Display Confusion Matrix for the test data.</span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polar2</span></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "confusion matrix for test data(rbf)"</span></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a>cm_test_rbf<span class="op">=</span>confusion_matrix(y_test,yp_test_rbf)</span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>disp_test_rbf <span class="op">=</span> ConfusionMatrixDisplay(confusion_matrix<span class="op">=</span>cm_test_rbf,display_labels<span class="op">=</span>model_linear.classes_)</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a>disp_test_rbf.plot()</span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a>From the classification report for test data and the consusion matrix, the accuracy of this SVM model with rbf kernal is 0.932039, which is slightly higher than the result from the linear model.</span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>From the above SVM model, we can see that the model is very effective in predicting sentiment analysis. After collecting a certain amount of data, the SVM model is more accurate than the NB model but is relatively time-consuming. </span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>SVM is a very powerful model and performs well on a variety of datasets. SVM allows decision bounds to be complex, even if the data has only a few features. It performs well on data with few features and many features, but does not perform well for scaling the number of samples. Running SVM on data with up to 10,000 samples may perform well, but if the amount of data reaches 1,000,000 rows or even larger, there may be challenges in terms of runtime and memory usage. another disadvantage of SVM is that pre-processing the data and tuning the parameters requires great care. For example, random forests require little or no preprocessing.</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a>Although the cryptocurrency market is highly risky, its advantages such as transaction speed, transaction costs, accessibility, security, etc. still make it a popular investment choice for many investors. Most of the investors are still positive about it.</span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pro and Cons</span></span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a>**Advantages**: good adaptation to high-dimensional spaces; good performance even when the number of data dimensions is larger than the number of samples; only a subset of the training set is required.</span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a>**Disadvantages**: SVM performs poorly if the number of features of the data is much larger than the number of samples; SVM does not directly provide quantitative likelihood predictions, these are usually achieved through cross-validation methods.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>