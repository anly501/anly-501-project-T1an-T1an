{
  "cells": [
    {
      "cell_type": "raw",
      "id": "99629616",
      "metadata": {},
      "source": [
        "---\n",
        "title: Decision Tree for Currency Data\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc621af2",
      "metadata": {},
      "source": [
        "### Method: Decision Tree\n",
        "\n",
        "A decision tree is a predictive model used in machine learning; it depicts the mapping relationship between object attributes and object values. Each leaf node in the tree corresponds to the value of the item represented by the path from the root node to that leaf node, whereas each diverging path in the tree represents a potential attribute value. Data mining methods that typically employ decision trees include data analysis and prediction.\n",
        "\n",
        "My data contains multiple columns with numerical values, such as open price, close price, volumn, etc. Our labels are divided into 0 and 1. 1 means that the price of the cryptocurrency moved more than 0.3 on the day, and 0 means that the price of the cryptocurrency moved less than 0.3.\n",
        "\n",
        "Since the data sample is not very large, decision trees can be used to analyze small and medium-sized data. Technical teams and stakeholders may easily understand a decision tree model since it is so obvious.\n",
        "\n",
        "*Library packages*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c141ff74",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "import random\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar\n",
        "#| fig-cap: Heatmap\n",
        "cur = pd.read_csv('../../data/01-modified-data/x_naive_bayes_r.csv')\n",
        "corr = cur.corr()\n",
        "# INSERT CODE TO SHOW A HEAT MAP FOR THE X FEATURES\n",
        "\n",
        "sns.set_theme(style=\"white\")\n",
        "f, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True) \t# Generate a custom diverging colormap\n",
        "\n",
        "\n",
        "\n",
        "sns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n",
        "        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar1\n",
        "#| fig-cap: Distribution of Class Labels\n",
        "y = pd.read_csv('../../data/01-modified-data/y_naive_bayes_r.csv')\n",
        "y.rename(columns={'0':'label'}, inplace=True)\n",
        "\n",
        "\n",
        "sns.displot(y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c73a9f",
      "metadata": {},
      "source": [
        "Figure 2 shows the distribution of the class labels. Theoretically, if our model is accurate enough, the distribution of the class labels of our predictions will be very close to figure 2.\n",
        "\n",
        "\n",
        "#### Baseline Model for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f79ce62",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_0 = y[y['label'] == 0].count().values\n",
        "y_1 = y[y['label'] == 1].count().values\n",
        "print('The number of 0 in the label is', y_0)\n",
        "print('The number of 1 in the label is', y_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d84144",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_label_data(class_labels, weights,N=334):\n",
        "    \n",
        "\n",
        "    y=random.choices(class_labels, weights = weights, k = N)\n",
        "    print(\"-----GENERATING DATA-----\")\n",
        "    print(\"unique entries:\",Counter(y).keys())  \n",
        "    print(\"count of labels:\",Counter(y).values()) # counts the elements' frequency\n",
        "    print(\"probability of labels:\",np.fromiter(Counter(y).values(), dtype=float)/len(y)) # counts the elements' frequency\n",
        "    return y\n",
        "\n",
        "#TEST\n",
        "y_ram=generate_label_data([0,1],[0.56,0.44],334)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-planets",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-planets\n",
        "#| tbl-cap: Classification Report Matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = y_ram\n",
        "y_true = y\n",
        "class_report_ran=classification_report(y_true,y_pred, output_dict=True)\n",
        "class_report_ran=pd.DataFrame(class_report_ran).transpose()\n",
        "class_report_ran"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04231bf5",
      "metadata": {},
      "source": [
        "#### Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7170674",
      "metadata": {},
      "outputs": [],
      "source": [
        "#find features : Tree-based feature selection\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "cur.shape\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=50)\n",
        "clf = clf.fit(cur, y)\n",
        "clf.feature_importances_  \n",
        "\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "X_new = model.transform(cur)\n",
        "X_new.shape\n",
        "res = [X_new[0],X_new[1]]\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd78833d",
      "metadata": {},
      "source": [
        "After finding the best feature(s), only variable 'close' was chosen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar2",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar2\n",
        "#| fig-cap: Pairplot\n",
        "feature_cols = ['close']\n",
        "X = cur[feature_cols]\n",
        "df = pd.concat([X,y], axis = 1)\n",
        "\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41616b4",
      "metadata": {},
      "source": [
        "*Split dataset into training set and test set*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf13046",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "print(\"x_train.shape\t\t:\",X_train.shape)\n",
        "print(\"y_train.shape\t\t:\",y_train.shape)\n",
        "\n",
        "print(\"X_test.shape\t\t:\",X_test.shape)\n",
        "print(\"y_test.shape\t\t:\",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9731af13",
      "metadata": {},
      "source": [
        "#### Model Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4929e2f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \n",
        "hyper_param=[]\n",
        "train_error=[]\n",
        "test_error=[]\n",
        "\n",
        "# LOOP OVER HYPER-PARAM\n",
        "for i in range(1,40):\n",
        "    # INITIALIZE MODEL \n",
        "    model = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # TRAIN MODEL \n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n",
        "    yp_train = model.predict(X_train)\n",
        "    yp_test = model.predict(X_test)\n",
        "\n",
        "    # shift=1+np.min(y_train) #add shift to remove division by zero \n",
        "    err1=mean_absolute_error(y_train, yp_train) \n",
        "    err2=mean_absolute_error(y_test, yp_test) \n",
        "    \n",
        "    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n",
        "    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n",
        "\n",
        "    hyper_param.append(i)\n",
        "    train_error.append(err1)\n",
        "    test_error.append(err2)\n",
        "\n",
        "    if(i==1 or i%10==0):\n",
        "        print(\"hyperparam =\",i)\n",
        "        print(\" train error:\",err1)\n",
        "        print(\" test error:\" ,err2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar3\n",
        "#| fig-cap: MAE vs Dept of Tree\n",
        "# Generate plot\n",
        "\n",
        "plt.plot(hyper_param,train_error ,linewidth=2, color='k')\n",
        "plt.plot(hyper_param,test_error ,linewidth=2, color='b')\n",
        "\n",
        "plt.xlabel(\"Depth of tree (max depth)\")\n",
        "plt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n",
        "\n",
        "i=1\n",
        "print(hyper_param[i],train_error[i],test_error[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar4\n",
        "#| fig-cap: Testing&Training Avvuracy vs Dept of Tree\n",
        "# Setup arrays to store train and test accuracies\n",
        "dep = np.arange(1, 15)\n",
        "train_accuracy = np.empty(len(dep))\n",
        "test_accuracy = np.empty(len(dep))\n",
        "\n",
        "# Loop over different values of k\n",
        "for i, k in enumerate(dep):\n",
        "    # Setup a Decision Tree Classifier\n",
        "    clf = tree.DecisionTreeClassifier(max_depth=k)\n",
        "\n",
        "    # Fit the classifier to the training data\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    #Compute accuracy on the training set\n",
        "    train_accuracy[i] = clf.score(X_train, y_train)\n",
        "\n",
        "    #Compute accuracy on the testing set\n",
        "    test_accuracy[i] = clf.score(X_test, y_test)\n",
        "\n",
        "# Generate plot\n",
        "\n",
        "plt.title('clf: Varying depth of tree')\n",
        "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
        "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Depth of tree')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae44df6",
      "metadata": {},
      "source": [
        "From fig4 and fig5, the model has the best accuracy when max_depth = 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4b2627",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=10)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "score = metrics.accuracy_score(y_test, y_pred)\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",round(score,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27bd3c5",
      "metadata": {},
      "source": [
        "*Training & Testing errors*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1312275",
      "metadata": {},
      "outputs": [],
      "source": [
        "# INITIALIZE MODEL \n",
        "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=10)\n",
        "model.fit(X_train,y_train)                     # TRAIN MODEL \n",
        "\n",
        "\n",
        "# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n",
        "yp_train = model.predict(X_train)\n",
        "yp_test = model.predict(X_test)\n",
        "\n",
        "err1=mean_absolute_error(y_train, yp_train) \n",
        "err2=mean_absolute_error(y_test, yp_test) \n",
        "    \n",
        "print(\" train error:\",round(err1,2))\n",
        "print(\" test error:\" ,round(err2,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3e5e8f1",
      "metadata": {},
      "source": [
        "*Plots of the Decision Tree*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-polar5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-polar5\n",
        "#| fig-cap: Decision Tree\n",
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(model, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "graph.write_png('../../501-project-website/images/decision-tree/decision-tree-currency.png')\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95850524",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "def confusion_plot(y_data,y_pred):\n",
        "    \n",
        "    print(\"ACCURACY:\", accuracy_score(y_data,y_pred))\n",
        "    print(\"NEGATIVE RECALL (Y=0):\",recall_score(y_data,y_pred,pos_label=0))\n",
        "    print(\"NEGATIVE PRECISION (Y=0):\",precision_score(y_data,y_pred,pos_label=0))\n",
        "    print(\"POSITIVE RECALL (Y=1):\",recall_score(y_data,y_pred,pos_label=1))\n",
        "    print(\"POSITIVE PRECISION (Y=1):\",precision_score(y_data,y_pred,pos_label=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe07428",
      "metadata": {},
      "source": [
        "*Classification Report Matrix*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-planets1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-planets1\n",
        "#| tbl-cap: Classification Report Matrix\n",
        "class_report_test=classification_report(y_test,yp_test, output_dict=True)\n",
        "class_report_test=pd.DataFrame(class_report_test).transpose()\n",
        "class_report_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145f4a1d",
      "metadata": {},
      "source": [
        "#### Final Results\n",
        "The accuracy after running the random classifier is 0.50.  From table 1, the final result is provided. The  model accuracy is 0.58, which is slightly higher than the accuracy from the random classifier, and the error rate is 0.42. This accuracy is not good enough because the price of crypocurrency changed dramatically in recent month which is quite difficult to predict.\n",
        "\n",
        "#### Conclusions\n",
        "Over the past year, the price of cryptocurrencies has been very volatile, so the accuracy of our model is not very high. Epidemic factors and changes in graphics card prices also have uncontrollable price movements on cryptocurrencies.\n",
        "\n",
        "\n",
        "There is a difference between cryptocurrencies and stocks. We can predict the stock market through company annual reports, market data and government policies, but the entire cryptocurrency market can fluctuate dramatically even by a single tweet. This trend also indicates that cryptocurrencies are not a good investment.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "62304a6ef618282fb40c3e2e0055fc428d677520f51f26b29ccb15084f5e1b94"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
